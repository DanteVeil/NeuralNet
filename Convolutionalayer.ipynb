# Import dependencies
import torch
from torch import nn, save, load
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import numpy as np

# Get data
# Training data
train = datasets.MNIST(root="data", download=True, train=True, transform=ToTensor())
dataset = DataLoader(train, 32)
# Test data
test = datasets.MNIST(root="data", download=True, train=False, transform=ToTensor())
test_loader = DataLoader(test, 32)

# Image Classifier NN
class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 32, (3,3)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (3,3)),
            nn.ReLU(),
            # FIX: Changed the output channels from 63 to 64 to match the Linear layer's input calculation
            nn.Conv2d(64, 64, (3,3)),
            nn.ReLU(),
            nn.Flatten(),
            # The input features are now correctly calculated as 64 * 22 * 22 = 30976
            nn.Linear(64 * (28 - 6) * (28 - 6), 10)
    )

    def forward(self, x):
        return self.model(x)

# Instance of neural network, optimizer, and loss function
# Check for available device (MPS for Apple Silicon, CUDA for Nvidia, otherwise CPU)
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")

clf = ImageClassifier().to(device)
opt = Adam(clf.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

# List to store loss values for plotting
loss_history = []

# Training flow
if __name__ == "__main__":
    for epoch in range(10): # Training for 10 epochs
        epoch_loss = 0.0
        for batch in dataset:
            X, y = batch
            X, y = X.to(device), y.to(device)
            yhat = clf(X)
            loss = loss_fn(yhat, y)

            # Backpropagation
            opt.zero_grad()
            loss.backward()
            opt.step()
            
            epoch_loss += loss.item()
        
        # Calculate average loss for the epoch and store it
        avg_loss = epoch_loss / len(dataset)
        loss_history.append(avg_loss)
        print(f"Epoch:{epoch} loss is {avg_loss}")

    # Save the trained model
    with open('model_state.pt', 'wb') as f:
        save(clf.state_dict(), f)

### --- Visualization and Prediction ---

# Plotting the training loss
plt.figure(figsize=(10, 5))
plt.plot(loss_history, label='Training Loss')
plt.title('Training Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Making predictions on the test set
clf.eval() # Set the model to evaluation mode
with torch.no_grad(): # Disable gradient calculation
    images, labels = next(iter(test_loader))
    images, labels = images.to(device), labels.to(device)
    
    predictions = clf(images)
    predicted_classes = torch.argmax(predictions, dim=1)

    # Move data to CPU for numpy and plotting
    images = images.cpu()
    labels = labels.cpu()
    predicted_classes = predicted_classes.cpu()

    # Display images and their predicted vs. actual labels
    plt.figure(figsize=(12, 12))
    for i in range(16): # Display the first 16 images of the batch
        plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].squeeze(), cmap="gray")
        plt.title(f"Pred: {predicted_classes[i].item()}, True: {labels[i].item()}",
                  color=("green" if predicted_classes[i] == labels[i] else "red"))
        plt.axis("off")
    plt.tight_layout()
    plt.show()
